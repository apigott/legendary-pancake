{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b3228ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RL imports\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from stable_baselines3 import PPO\n",
    "import supersuit as ss\n",
    "from pettingzoo import ParallelEnv\n",
    "from pettingzoo.test import parallel_api_test\n",
    "import gym\n",
    "\n",
    "# other useful stuff\n",
    "import pandas as pd\n",
    "import random, string\n",
    "import numpy as np\n",
    "from copy import deepcopy, copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e00d2f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this represents the CityLearn grid\n",
    "class MyGrid:\n",
    "    def __init__(self, dataframe, nclusters):\n",
    "        self.nclusters = nclusters\n",
    "        self.df = dataframe # analogous to the pandapower network (dataframes)\n",
    "        self.possible_agents = list(self.df['name'])\n",
    "        self.clusters = self.set_clusters()\n",
    "        self.ts = 0\n",
    "\n",
    "    def set_clusters(self):\n",
    "        clusters = []\n",
    "        for i in range(self.nclusters): # let every other agent be on the opposing team\n",
    "            clusters += [self.possible_agents[i::self.nclusters]]\n",
    "        return clusters\n",
    "\n",
    "    def get_action_space(self, agents):\n",
    "        # return the action space of each agent {agent: gym.space(data)}\n",
    "        aspace = {agent: gym.spaces.Box(low=-1*np.ones(1), high=np.ones(1)) for agent in agents}\n",
    "        return aspace\n",
    "\n",
    "    def get_observation_space(self, agents):\n",
    "        # return the observation space of each agent {agent: gym.space(data)}\n",
    "        ospace = {agent: gym.spaces.Box(low=-1*np.ones(1), high=np.ones(1)) for agent in agents}\n",
    "        return ospace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b47a9d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a wrapper for the grid environment\n",
    "class MyEnv(ParallelEnv):\n",
    "    def __init__(self, grid):\n",
    "        # initialize env with one team of agents\n",
    "        self.agents = grid.clusters.pop()\n",
    "        self.possible_agents = self.agents[:]\n",
    "        # and get the corresponding actionspaces\n",
    "        self.action_spaces = grid.get_action_space(self.agents)\n",
    "        self.observation_spaces = grid.get_observation_space(self.agents)\n",
    "\n",
    "        self.metadata = {'render.modes': [], 'name':\"my_env\"} # gym stuff, not used\n",
    "        self.ts = 0 # if we want to track num. steps these agents have taken\n",
    "\n",
    "    def reset(self):\n",
    "        # reset the environment to something randomized\n",
    "        for agent in self.agents:\n",
    "            self.grid.df.loc[self.grid.df.name==agent, 'observation'] = np.random.uniform(0,1)\n",
    "        return self.state()\n",
    "\n",
    "    def state(self):\n",
    "        # return the observed state {agent: np.array(data)}\n",
    "        obs = {agent: np.array(float(self.grid.df.loc[self.grid.df.name==agent, 'observation'])) for agent in self.agents}\n",
    "        return obs\n",
    "\n",
    "    def get_reward(self):\n",
    "        # return reward {agent: float(data)}\n",
    "        # in this case, reward = state = action\n",
    "        rewards = {agent: float(self.grid.df.loc[self.grid.df.name==agent, 'observation']) for agent in self.agents}\n",
    "        return rewards\n",
    "\n",
    "    def get_done(self):\n",
    "        # environment never ends, return {agent: bool(data)}\n",
    "        dones = {agent: False for agent in self.agents}\n",
    "        return dones\n",
    "\n",
    "    def get_info(self):\n",
    "        # no additional info to track\n",
    "        infos = {agent: {} for agent in self.agents}\n",
    "        return infos\n",
    "\n",
    "    def step(self, action_dict):\n",
    "        # input: actions to take = {agent: action}\n",
    "        # output: dictionaries of format {agent: data} for obs, reward, done, info\n",
    "        for agent in action_dict.keys():\n",
    "            self.grid.df.loc[self.grid.df.name==agent, 'observation'] = action_dict[agent]\n",
    "\n",
    "        print(self.grid.df) # check how the dataframe evolves\n",
    "        self.grid.ts += 1\n",
    "        self.ts += 1\n",
    "        return self.state(), self.get_reward(), self.get_done(), self.get_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "646134fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/panda/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "# instantiate all the objects...\n",
    "# create a dumb dataframe so we can see how the agents interact\n",
    "agents = ['a','b','c','d'] # 4 agents, 2 teams\n",
    "nteams = 2 # number of teams\n",
    "df = pd.DataFrame({'name':agents,'observation':[np.random.uniform(0,2) for _ in range(len(agents))]})\n",
    "grid = MyGrid(df,nteams)\n",
    "\n",
    "# create some parallel environments\n",
    "teams = [MyEnv(grid), MyEnv(grid)]\n",
    "# cast them to pettingzoo envs (so we can train multiple agents on them)\n",
    "teams = [ss.pettingzoo_env_to_vec_env_v0(team) for team in teams]\n",
    "# copy environment and stack it so we can use PPO and get [nenvs] sets of data at once\n",
    "# each team trains on 2 environments \"in parallel\"\n",
    "nenvs = 2\n",
    "teams = [ss.concat_vec_envs_v0(team, nenvs, num_cpus=1, base_class='stable_baselines3') for team in teams]\n",
    "\n",
    "# set the grid after initialization, this way the object is shared across env instances\n",
    "# this also saves RAM by only copying the grid when we need it\n",
    "# (otherwise concat_vec_envs_v0 will copy it nteams*nenvs times)\n",
    "grids = [deepcopy(grid) for n in range(nenvs)]\n",
    "for team in teams:\n",
    "    for n in range(nenvs):\n",
    "        team.venv.vec_envs[n].par_env.grid = grids[n]\n",
    "\n",
    "# create some stable baselines models\n",
    "# batch_size = nenvs*nsteps and MUST be greater than 1\n",
    "models = [PPO(MlpPolicy, team, verbose=2, gamma=0.999, batch_size=2, n_steps=1, ent_coef=0.01, learning_rate=0.00025, vf_coef=0.5, max_grad_norm=0.5, gae_lambda=0.95, n_epochs=4, clip_range=0.2, clip_range_vf=1) for team in teams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b025899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  name  observation\n",
      "0    a     0.446070\n",
      "1    b     0.049186\n",
      "2    c     1.519710\n",
      "3    d     0.939704\n",
      "  name  observation\n",
      "0    a     0.446070\n",
      "1    b     1.000000\n",
      "2    c     1.519710\n",
      "3    d     0.355373\n",
      "----------------------------\n",
      "| time/              |     |\n",
      "|    fps             | 263 |\n",
      "|    iterations      | 1   |\n",
      "|    time_elapsed    | 0   |\n",
      "|    total_timesteps | 4   |\n",
      "----------------------------\n",
      "  name  observation\n",
      "0    a    -1.000000\n",
      "1    b     0.049186\n",
      "2    c    -1.000000\n",
      "3    d     0.939704\n",
      "  name  observation\n",
      "0    a     1.000000\n",
      "1    b     1.000000\n",
      "2    c    -0.374641\n",
      "3    d     0.355373\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 360          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 0            |\n",
      "|    total_timesteps      | 4            |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | -0.017018646 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    clip_range_vf        | 1            |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -0.039       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.172        |\n",
      "|    n_updates            | 4            |\n",
      "|    policy_gradient_loss | -0.00434     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.429        |\n",
      "------------------------------------------\n",
      "  name  observation\n",
      "0    a    -1.000000\n",
      "1    b    -1.000000\n",
      "2    c    -1.000000\n",
      "3    d     0.216454\n",
      "  name  observation\n",
      "0    a     1.000000\n",
      "1    b     1.000000\n",
      "2    c    -0.374641\n",
      "3    d     0.885763\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 783          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 0            |\n",
      "|    total_timesteps      | 8            |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | -0.012111604 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    clip_range_vf        | 1            |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -0.024       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.366        |\n",
      "|    n_updates            | 4            |\n",
      "|    policy_gradient_loss | 0.000689     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.684        |\n",
      "------------------------------------------\n",
      "  name  observation\n",
      "0    a    -1.000000\n",
      "1    b    -1.000000\n",
      "2    c    -0.590572\n",
      "3    d     0.216454\n",
      "  name  observation\n",
      "0    a    -0.119210\n",
      "1    b     1.000000\n",
      "2    c    -1.000000\n",
      "3    d     0.885763\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 586          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 0            |\n",
      "|    total_timesteps      | 8            |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | -0.028760761 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    clip_range_vf        | 1            |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.0518       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.22         |\n",
      "|    n_updates            | 8            |\n",
      "|    policy_gradient_loss | 0.00639      |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.621        |\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# train the models\n",
    "for ts in range(2):\n",
    "    for model in models: # each timestep alternate through models to take turns\n",
    "        model.learn(1, reset_num_timesteps=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d004eb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the models\n",
    "for ts in range(100):\n",
    "    for model in models: # each timestep alternate through models to take turns\n",
    "        model.learn(1, reset_num_timesteps=False)\n",
    "\n",
    "# reset the models to test them\n",
    "obss = [team.reset() for team in teams]\n",
    "for ts in range(5): # test on 5 timesteps\n",
    "    for m in range(len(models)): # again, alternate through models\n",
    "\n",
    "        # get the current observation from the perspective of the active team\n",
    "        # this can probably be cleaned up\n",
    "        foo = []\n",
    "        for e in range(nenvs):\n",
    "            bar = list(teams[m].venv.vec_envs[e].par_env.state().values())\n",
    "            foo += bar # may need additional logic to pad state/obs spaces if they aren't identical\n",
    "\n",
    "        foo = np.vstack(foo)\n",
    "        obss[m] = np.vstack(foo)\n",
    "\n",
    "        action = models[m].predict(obss[m])[0] # send it to the SB model to select an action\n",
    "        obss[m], reward, done, info = teams[m].step(action) # update environment\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "panda",
   "language": "python",
   "name": "panda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
